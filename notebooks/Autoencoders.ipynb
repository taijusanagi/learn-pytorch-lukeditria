{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://github.com/LukeDitria/pytorch_tutorials/blob/main/section07_autoencoders/solutions/Pytorch1_Autoencoders_Solutions.ipynb\n",
    "\n",
    "## Summary\n",
    "\n",
    "This code uses an autoencoder for denoising. It applies convolutional downsampling and transposed-convolution upsampling to compress and reconstruct images. The network is explicitly trained to remove noise by mapping noisy inputs back to clean targets. It is not a generative model, so sampling or generating new images from random latent vectors does not work well.\n",
    "\n",
    "## Note\n",
    "\n",
    "When an autoencoder is used for feature reduction, the model is trained by feeding in clean images and computing the loss against those same clean images. The latent space can be used for downstream tasks like classification, but it is not suitable for generative use. See Variational-Autoencoders for generative use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Define learning rate\n",
    "lr = 1e-4\n",
    "\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "\n",
    "# Dataset location\n",
    "root = \"../datasets\"\n",
    "\n",
    "# Scale for the added image noise\n",
    "noise_scale = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, z=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, ch, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, 2 * ch, 3, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.Conv2d(2 * ch, 4 * ch, 3, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(4 * ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        return self.conv_out(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(4 * ch)\n",
    "        self.conv2 = nn.ConvTranspose2d(4 * ch, 2 * ch, 3, 2, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.ConvTranspose2d(2 * ch, ch, 3, 2, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch)\n",
    "        self.conv4 = nn.ConvTranspose2d(ch, ch, 3, 2, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))\n",
    "    \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, z=32):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        x = self.decoder(encoding)\n",
    "        return x, encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "# Fixed version issue\n",
    "test_images = next(dataiter)[0]\n",
    "# View the shape\n",
    "test_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20, 10))\n",
    "random_sample = (torch.bernoulli((1 - noise_scale) * torch.ones_like(test_images)) * 2) - 1\n",
    "noisy_test_img = random_sample * test_images\n",
    "\n",
    "out = vutils.make_grid(noisy_test_img[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the Latent Vector\n",
    "latent_size = 128\n",
    "\n",
    "# Create our network\n",
    "ae_net = AE(channel_in=1, z=latent_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(ae_net.parameters(), lr=lr)\n",
    "\n",
    "# MSE loss for reconstruction!\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "loss_log = []\n",
    "train_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, encoding = ae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "encoding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, nepoch, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "        \n",
    "        random_sample = (torch.bernoulli((1 - noise_scale) * torch.ones_like(image)) * 2) - 1\n",
    "        noisy_img = random_sample * image\n",
    "        \n",
    "        # Forward pass the image in the data tuple\n",
    "        recon_data, _ = ae_net(noisy_img)\n",
    "        \n",
    "        # Calculate the MSE loss\n",
    "        loss = loss_func(recon_data, image)\n",
    "        \n",
    "        # Log the loss\n",
    "        loss_log.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Take a training step\n",
    "        ae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_log)\n",
    "_ = plt.title(\"MSE Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(noisy_test_img[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "plt.figure(figsize = (20,10))\n",
    "recon_data, encoding = ae_net(noisy_test_img.to(device))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "plt.figure(figsize = (20,10))\n",
    "recon_data = ae_net.decoder(encoding.std(0, keepdims=True) * torch.randn_like(encoding) + encoding.mean(0, keepdims=True))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
