{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f082db07-b6a9-457c-b1c1-8426c842641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e209441",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Define learning rate\n",
    "lr = 1e-4\n",
    "\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "\n",
    "# Dataset location\n",
    "root = \"../datasets\"\n",
    "\n",
    "# Scale for the added image noise\n",
    "noise_scale = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add940e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ca59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:19<00:00, 508kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 112kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:09<00:00, 178kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, z=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, ch, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, 2 * ch, 3, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.Conv2d(2 * ch, 4 * ch, 3, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(4 * ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        return self.conv_out(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(4 * ch)\n",
    "        self.conv2 = nn.ConvTranspose2d(4 * ch, 2 * ch, 3, 2, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.ConvTranspose2d(2 * ch, ch, 3, 2, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch)\n",
    "        self.conv4 = nn.ConvTranspose2d(ch, ch, 3, 2, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))\n",
    "    \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, z=32):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        x = self.decoder(encoding)\n",
    "        return x, encoding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
